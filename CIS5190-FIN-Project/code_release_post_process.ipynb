{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5638b0d1",
   "metadata": {
    "id": "5638b0d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting exifread\n",
      "  Downloading ExifRead-3.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Downloading ExifRead-3.0.0-py3-none-any.whl (40 kB)\n",
      "Installing collected packages: exifread\n",
      "Successfully installed exifread-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install exifread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yT4UQMosNN02",
   "metadata": {
    "id": "yT4UQMosNN02"
   },
   "source": [
    "# Extracting GPS Information from Images\n",
    "\n",
    "(You will need to modify this script based on how your dataset is stored in order to execute the code.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7QcjgXtAQB5Q",
   "metadata": {
    "id": "7QcjgXtAQB5Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# train_or_test_or_validation could be either train, test, or validation.\n",
    "train_or_test_or_validation = \"train\" # it could also be test or validation\n",
    "PATH_TO_YOUR_DATA_FOLDER = \"~/Desktop/CIS519\"\n",
    "directory_path = f\"{PATH_TO_YOUR_DATA_FOLDER}/{train_or_test_or_validation}\"\n",
    "output_csv = \"metadata.csv\"\n",
    "output_csv = os.path.join(directory_path, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1713e4e-a848-42e9-b8d8-d1273f1b9689",
   "metadata": {
    "id": "e1713e4e-a848-42e9-b8d8-d1273f1b9689"
   },
   "outputs": [],
   "source": [
    "import exifread, csv\n",
    "\n",
    "def get_exif_data(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        tags = exifread.process_file(image_file)\n",
    "    return tags\n",
    "\n",
    "def export_exif_to_json(exif_data, output_file):\n",
    "    # Convert tags to a serializable format\n",
    "    exif_data_serializable = {str(tag): str(value) for tag, value in exif_data.items()}\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(exif_data_serializable, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fb378d",
   "metadata": {
    "id": "80fb378d"
   },
   "outputs": [],
   "source": [
    "# Function to convert GPS coordinates in degrees, minutes, and seconds to decimal degrees\n",
    "def convert_to_decimal_degrees(value):\n",
    "    d, m, s = value.values\n",
    "    return d.num / d.den + (m.num / m.den) / 60 + (s.num / s.den) / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yaoJPVKrNq9N",
   "metadata": {
    "id": "yaoJPVKrNq9N"
   },
   "source": [
    "### You will need to create subfolders in {PATH_TO_YOUR_DATA_FOLDER} for each split (train/test/validation) or just (train/test). Next, place the corresponding images into each split after randomly shuffling them. Then, create a metadata.csv file for each split and place it in the corresponding directory. Note that the current code only works for jpeg images. If the exported images are in some other format, convert them to .jpg before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c347d",
   "metadata": {
    "id": "be3c347d"
   },
   "outputs": [],
   "source": [
    "with open(output_csv, mode='w', newline='') as csv_file:\n",
    "    fieldnames = ['file_name', 'Latitude', 'Longitude']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "            exif_data = get_exif_data(os.path.join(directory_path, filename))\n",
    "            if exif_data:\n",
    "                gps_latitude = exif_data.get('GPS GPSLatitude', None)\n",
    "                gps_latitude_ref = exif_data.get('GPS GPSLatitudeRef', None)\n",
    "                gps_longitude = exif_data.get('GPS GPSLongitude', None)\n",
    "                gps_longitude_ref = exif_data.get('GPS GPSLongitudeRef', None)\n",
    "                if gps_latitude and gps_longitude:\n",
    "                    # Convert latitude and longitude to decimal degrees\n",
    "                    latitude = convert_to_decimal_degrees(gps_latitude)\n",
    "                    longitude = convert_to_decimal_degrees(gps_longitude)\n",
    "\n",
    "                    # Adjust for N/S and E/W reference\n",
    "                    if gps_latitude_ref.values[0] == 'S':\n",
    "                        latitude = -latitude\n",
    "                    if gps_longitude_ref.values[0] == 'W':\n",
    "                        longitude = -longitude\n",
    "\n",
    "                    # Write the data to the CSV file\n",
    "                    writer.writerow({'file_name': filename, 'Latitude': latitude, 'Longitude': longitude})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WEkVLwUcQ7PV",
   "metadata": {
    "id": "WEkVLwUcQ7PV"
   },
   "source": [
    "# Uploading and Reading a Dataset on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xtmhkAUSRBxp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtmhkAUSRBxp",
    "outputId": "cab82646-49d6-431e-bb91-a5fee48296f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dkkEth2_Ui18",
   "metadata": {
    "id": "dkkEth2_Ui18"
   },
   "source": [
    "### You need to ensure that your Hugging Face token has both read and write access to your repository and Hugging Face organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ZHlkuzFUhDX",
   "metadata": {
    "id": "5ZHlkuzFUhDX"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quS-mt_9RIpd",
   "metadata": {
    "id": "quS-mt_9RIpd"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=PATH_TO_YOUR_DATA_FOLDER)\n",
    "\n",
    "dataset.push_to_hub(\"{YOUR ORGANIZATION NAME}/{DATASET NAME}\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o9Cyq_g9Wf8h",
   "metadata": {
    "id": "o9Cyq_g9Wf8h"
   },
   "outputs": [],
   "source": [
    "### After uploading, you can access your data using this code\n",
    "dataset_test = load_dataset(\"{YOUR ORGANIZATION NAME}/{DATASET NAME}\", split=\"{YOUR SPLIT}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "initial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
